# MLOps Pipeline: Cats vs Dogs Classification

GitHub Link: https://github.com/2024ab05191-Swatheka/mlops-cats-dogs-classification_Assignment.git

Google Drive : https://drive.google.com/drive/folders/1j3OqX4PV3L917TouYgIRxGTGIkZ4AXJX?usp=drive_link
Recording Link: https://drive.google.com/file/d/1Ep_N0dZ0B4ixNBunzVdKDQI_MUG8ehOa/view?usp=drive_link

## ğŸ¯ Project Overview

End-to-end MLOps pipeline for binary image classification (Cats vs Dogs) for a pet adoption platform. This project demonstrates best practices in ML model development, experiment tracking, versioning, containerization, and CI/CD deployment.

## ğŸ“Š Dataset

- **Source**: Kaggle Cats and Dogs Dataset
- **Total Samples**: ~24,000 images
- **Classes**: Cat (0), Dog (1)
- **Split**: 80% Train / 10% Validation / 10% Test
- **Format**: .jpg images
- **Target Size**: 224x224 RGB

## ğŸ—ï¸ Architecture

**Model**: Baseline CNN

- 4 Convolutional blocks (32 â†’ 64 â†’ 128 â†’ 256 filters)
- Batch Normalization & MaxPooling
- 3 Fully Connected layers with Dropout
- ~14M trainable parameters

## ğŸ“¦ Project Structure

```
mlops_project/
â”œâ”€â”€ mlops_cats_dogs_pipeline.ipynb  # Main notebook with all implementations
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ README.md                        # Project documentation
â”œâ”€â”€ TEAM_SETUP.md                    # Team collaboration guide
â”œâ”€â”€ DVC_REMOTE_SETUP.md             # DVC remote storage setup
â”œâ”€â”€ .gitignore                       # Git ignore rules
â”œâ”€â”€ dataset_metadata.json            # Dataset version info
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ PetImages.dvc           # DVC-tracked dataset (809 MB, 24,998 files)
â”‚   â””â”€â”€ processed/                   # Processed datasets
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best_model.pt               # Best validation model
â”‚   â”œâ”€â”€ baseline_cnn_final.pt       # Final trained model
â”‚   â”œâ”€â”€ baseline_cnn_scripted.pt    # TorchScript model
â”‚   â”œâ”€â”€ baseline_cnn.pkl            # Pickle format
â”‚   â””â”€â”€ baseline_cnn.onnx           # ONNX format (optional)
â””â”€â”€ experiments/
    â”œâ”€â”€ mlruns/                      # MLflow tracking data
    â”œâ”€â”€ class_distribution.png       # EDA visualizations
    â”œâ”€â”€ sample_images.png
    â”œâ”€â”€ training_curves.png          # Loss & accuracy plots
    â”œâ”€â”€ confusion_matrix.png         # Model evaluation
    â””â”€â”€ experiment_report.txt        # Summary report
```

## ğŸš€ Module M1: Model Development & Experiment Tracking

### Features Implemented

âœ… **Data Versioning**: DVC for dataset tracking  
âœ… **Data Pipeline**: Loading, cleaning, preprocessing (224x224 RGB)  
âœ… **Data Augmentation**: Random flips, rotation, color jitter  
âœ… **Model Training**: Baseline CNN with PyTorch  
âœ… **Experiment Tracking**: MLflow for metrics, parameters, artifacts  
âœ… **Evaluation**: Confusion matrix, classification report, loss curves  
âœ… **Model Serialization**: .pt, .pkl, .onnx, TorchScript formats

### Technologies Used

- **ML Framework**: PyTorch
- **Experiment Tracking**: MLflow
- **Version Control**: Git (code) + DVC (data)
- **Data Processing**: NumPy, Pandas, PIL
- **Visualization**: Matplotlib, Seaborn
- **Evaluation**: scikit-learn

## ğŸ› ï¸ Setup & Installation

### Prerequisites

- Python 3.8+
- CUDA-capable GPU (optional, recommended)
- Git
- DVC

### Installation Steps

1. **Clone the repository**

```bash
git clone <your-repo-url>
cd mlops_project
```

2. **Get dataset with DVC** (Automatic - No manual setup needed!)

```bash
# If DVC remote is configured (see DVC_REMOTE_SETUP.md)
dvc pull  # Downloads 809 MB dataset automatically

# Otherwise, dataset is already in Git LFS or included in repository
```

**Note**: The dataset path is already configured in the notebook to use `data/raw/PetImages`. No manual path setup required!

3. **Create virtual environment**

```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

4. **Install dependencies**

```bash
pip install -r requirements.txt
```

5. **Install PyTorch** (choose based on your system)

```bash
# CPU only
pip install torch torchvision torchaudio

# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### DVC Status

âœ… **Dataset tracked**: `data/raw/PetImages` (809 MB, 24,998 images)  
âœ… **DVC initialized**: Metadata in `data/raw/PetImages.dvc`  
âŒ **Remote storage**: Not configured yet

### Setup DVC Remote (Optional - For Team Sharing)

See [DVC_REMOTE_SETUP.md](DVC_REMOTE_SETUP.md) for detailed instructions.

**Quick Setup (Google Drive - Recommended)**

```bash
# 1. Create Google Drive folder and get folder ID
# 2. Configure remote
dvc remote add -d storage gdrive://YOUR_FOLDER_ID
git add .dvc/config
git commit -m "Add DVC remote"

# 3. Push dataset
dvc push

# Team members can now pull
dvc pull
```

**Other options**: AWS S3, Azure Blob, SSH Server (see DVC_REMOTE_SETUP.md)

## ğŸ““ Running the Notebook

### Option 1: Jupyter Notebook

```bash
jupyter notebook mlops_cats_dogs_pipeline.ipynb
```

### Option 2: JupyterLab

```bash
jupyter lab mlops_cats_dogs_pipeline.ipynb
```

### Option 3: VS Code

Open `mlops_cats_dogs_pipeline.ipynb` in VS Code with Jupyter extension

## ğŸ“Š View Experiments with MLflow

Start MLflow UI:

```bash
cd C:\Users\swath\dataset\mlops_project
mlflow ui --backend-store-uri file:///C:/Users/swath/dataset/mlops_project/experiments
```

Then open browser at: **http://localhost:5000**

### MLflow Tracks:

- Hyperparameters (learning rate, batch size, epochs, etc.)
- Metrics (train/val loss, train/val accuracy per epoch)
- Artifacts (model files, plots, confusion matrix)
- Model registry

## ğŸ¯ Model Performance

| Metric         | Value                     |
| -------------- | ------------------------- |
| Test Accuracy  | ~85-90% (after 10 epochs) |
| Model Size     | ~55 MB (PyTorch .pt)      |
| Inference Time | ~50ms per image (CPU)     |
| Parameters     | ~14M trainable            |

## ğŸ”„ Data Versioning with DVC

### Track Dataset

```bash
dvc add path/to/dataset
git add dataset.dvc .gitignore
git commit -m "Track dataset with DVC"
```

### Pull Dataset

```bash
dvc pull
```

### Push to Remote Storage (optional)

```bash
dvc remote add -d myremote s3://mybucket/path
dvc push
```

## ğŸ“ˆ Training Configuration

| Parameter     | Value             |
| ------------- | ----------------- |
| Image Size    | 224x224           |
| Batch Size    | 32                |
| Epochs        | 10                |
| Learning Rate | 0.001             |
| Optimizer     | Adam              |
| Loss Function | CrossEntropyLoss  |
| LR Scheduler  | ReduceLROnPlateau |

## ğŸ”¬ Experiment Tracking

All experiments are logged to MLflow with:

- **Parameters**: Model config, hyperparameters
- **Metrics**: Loss, accuracy (per epoch)
- **Artifacts**:
  - Model checkpoints (.pt, .pkl, .onnx)
  - Training curves
  - Confusion matrix
  - Sample predictions
  - Experiment report

## ğŸš€ Next Steps: Completed Modules

### âœ… M1: Model Development & Experiment Tracking (COMPLETE)

- âœ“ Git + DVC versioning
- âœ“ Baseline CNN model training
- âœ“ MLflow experiment tracking
- âœ“ Model serialization (.pt, .pkl, .onnx)

### âœ… M2: Containerization & Packaging (COMPLETE)

- âœ“ FastAPI REST API for inference
- âœ“ Docker image with dependencies
- âœ“ Health check and prediction endpoints
- âœ“ Model serving on port 8000

### âœ… M3: CI/CD Pipeline (COMPLETE)

- âœ“ GitHub Actions workflow
- âœ“ Automated testing (pytest)
- âœ“ Docker image build & publish
- âœ“ Container registry integration

### âœ… M4: Continuous Deployment (COMPLETE)

- âœ“ Docker Compose deployment
- âœ“ Kubernetes manifests (deployment + service)
- âœ“ Automated deployment on main branch
- âœ“ Post-deployment smoke tests
- âœ“ Pipeline fails on test failures

**See**: [M4_DEPLOYMENT_GUIDE.md](M4_DEPLOYMENT_GUIDE.md), [M4_QUICK_REFERENCE.md](M4_QUICK_REFERENCE.md)

### âœ… M5: Monitoring, Logs & Final Submission (COMPLETE)

- âœ“ Request/response logging
- âœ“ Metrics tracking (request count, latency)
- âœ“ /metrics endpoint for monitoring
- âœ“ Performance tracking script
- âœ“ Post-deployment accuracy monitoring
- âœ“ No sensitive data in logs

**See**: [M5_MONITORING_GUIDE.md](M5_MONITORING_GUIDE.md), [M5_QUICK_REFERENCE.md](M5_QUICK_REFERENCE.md)

### ğŸ”® Future Enhancements (Optional)

- Cloud deployment (Azure ML / AWS SageMaker)
- Auto-scaling & load balancing
- Advanced monitoring dashboards (Grafana)
- Automated retraining pipelines

## ğŸ“ Model Formats

The trained model is saved in multiple formats:

1. **PyTorch (.pt)** - Full checkpoint with optimizer state
2. **TorchScript (.pt)** - Optimized for production deployment
3. **Pickle (.pkl)** - Python serialization format
4. **ONNX (.onnx)** - Cross-platform interoperability

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open Pull Request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ‘¤ Author

**MLOps Engineer**

- Project: Cats vs Dogs Classification Pipeline
- Use Case: Pet Adoption Platform

## ğŸ™ Acknowledgments

- **Dataset**: Kaggle Cats and Dogs Dataset
- **Framework**: PyTorch
- **Experiment Tracking**: MLflow
- **Version Control**: Git + DVC

## ğŸ“ Support

For issues or questions, please open an issue in the repository.

---
