{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e6434d",
   "metadata": {},
   "source": [
    "# MLOps Pipeline: Cats vs Dogs Classification\n",
    "## M1: Model Development & Experiment Tracking\n",
    "\n",
    "### Use Case\n",
    "Binary image classification (Cats vs Dogs) for a pet adoption platform.\n",
    "\n",
    "### Dataset\n",
    "- **Source**: Kaggle Cats and Dogs classification dataset\n",
    "- **Preprocessing**: 224x224 RGB images for standard CNNs\n",
    "- **Split**: 80% train / 10% validation / 10% test\n",
    "- **Augmentation**: Random flips, rotation, color jitter for better generalization\n",
    "\n",
    "### M1 Objectives\n",
    "1. **Data & Code Versioning**: Git (code) + DVC (data)\n",
    "2. **Model Building**: Baseline CNN model with serialization (.pt, .pkl)\n",
    "3. **Experiment Tracking**: MLflow for logging runs, parameters, metrics, and artifacts (confusion matrix, loss curves)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf941fb4",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6df66b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (2.9.0+cu130)\n",
      "Requirement already satisfied: torchvision in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (0.24.0+cu130)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (2.9.0+cu130)\n",
      "Requirement already satisfied: filelock in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from torchvision) (2.3.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Downloading mlflow-3.9.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting dvc\n",
      "  Downloading dvc-3.66.1-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (11.3.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (4.67.1)\n",
      "Collecting mlflow-skinny==3.9.0 (from mlflow)\n",
      "  Downloading mlflow_skinny-3.9.0-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting mlflow-tracing==3.9.0 (from mlflow)\n",
      "  Downloading mlflow_tracing-3.9.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting Flask-CORS<7 (from mlflow)\n",
      "  Downloading flask_cors-6.0.2-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting Flask<4 (from mlflow)\n",
      "  Downloading flask-3.1.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
      "  Downloading alembic-1.18.4-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting cryptography<47,>=43.0.0 (from mlflow)\n",
      "  Downloading cryptography-46.0.5-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting huey<3,>=2.5.4 (from mlflow)\n",
      "  Downloading huey-2.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: pyarrow<23,>=4.0.0 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from mlflow) (22.0.0)\n",
      "Requirement already satisfied: scipy<2 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from mlflow) (1.16.3)\n",
      "Collecting skops<1 (from mlflow)\n",
      "  Downloading skops-0.13.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting sqlalchemy<3,>=1.4.0 (from mlflow)\n",
      "  Downloading sqlalchemy-2.0.46-cp314-cp314-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting waitress<4 (from mlflow)\n",
      "  Downloading waitress-3.0.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting cachetools<7,>=5.0.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading cachetools-6.2.6-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting click<9,>=7.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting cloudpickle<4 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading cloudpickle-3.1.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading databricks_sdk-0.88.0-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting fastapi<1 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading fastapi-0.129.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading gitpython-3.1.46-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting importlib_metadata!=4.7.0,<9,>=3.7.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading importlib_metadata-8.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-proto<3,>=1.9.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging<26 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow) (25.0)\n",
      "Collecting protobuf<7,>=3.12.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading protobuf-6.33.5-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting pydantic<3,>=2.0.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting python-dotenv<2,>=0.19.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow) (6.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow) (2.32.5)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading sqlparse-0.5.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from mlflow-skinny==3.9.0->mlflow) (4.15.0)\n",
      "Collecting uvicorn<1 (from mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading uvicorn-0.40.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from matplotlib) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from click<9,>=7.0->mlflow-skinny==3.9.0->mlflow) (0.4.6)\n",
      "Collecting cffi>=2.0.0 (from cryptography<47,>=43.0.0->mlflow)\n",
      "  Downloading cffi-2.0.0-cp314-cp314-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading google_auth-2.48.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pywin32>=304 (from docker<8,>=4.0.0->mlflow)\n",
      "  Downloading pywin32-311-cp314-cp314-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
      "Collecting starlette<1.0.0,>=0.40.0 (from fastapi<1->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading starlette-0.52.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from fastapi<1->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi<1->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting blinker>=1.9.0 (from Flask<4->mlflow)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting itsdangerous>=2.2.0 (from Flask<4->mlflow)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: jinja2>=3.1.2 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from Flask<4->mlflow) (3.1.6)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from Flask<4->mlflow) (2.1.5)\n",
      "Collecting werkzeug>=3.1.0 (from Flask<4->mlflow)\n",
      "  Downloading werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting zipp>=3.20 (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3,>=2.0.0->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading pydantic_core-2.41.5-cp314-cp314-win_amd64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.9.0->mlflow) (2025.10.5)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.9.0->mlflow)\n",
      "  Downloading pyasn1-0.6.2-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting prettytable>=3.9 (from skops<1->mlflow)\n",
      "  Downloading prettytable-3.17.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting greenlet>=1 (from sqlalchemy<3,>=1.4.0->mlflow)\n",
      "  Downloading greenlet-3.3.1-cp314-cp314-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from starlette<1.0.0,>=0.40.0->fastapi<1->mlflow-skinny==3.9.0->mlflow) (4.11.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<1.0.0,>=0.40.0->fastapi<1->mlflow-skinny==3.9.0->mlflow) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from uvicorn<1->mlflow-skinny==3.9.0->mlflow) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from dvc) (25.4.0)\n",
      "Collecting celery (from dvc)\n",
      "  Downloading celery-5.6.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting configobj>=5.0.9 (from dvc)\n",
      "  Downloading configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting distro>=1.3 (from dvc)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting dpath<3,>=2.1.0 (from dvc)\n",
      "  Downloading dpath-2.2.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting dulwich (from dvc)\n",
      "  Downloading dulwich-1.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dvc-data<3.19.0,>=3.18.0 (from dvc)\n",
      "  Downloading dvc_data-3.18.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting dvc-http>=2.29.0 (from dvc)\n",
      "  Downloading dvc_http-2.32.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting dvc-objects (from dvc)\n",
      "  Downloading dvc_objects-5.2.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting dvc-render<2,>=1.0.1 (from dvc)\n",
      "  Downloading dvc_render-1.0.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dvc-studio-client<1,>=0.21 (from dvc)\n",
      "  Downloading dvc_studio_client-0.22.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting dvc-task<1,>=0.3.0 (from dvc)\n",
      "  Downloading dvc_task-0.40.2-py3-none-any.whl.metadata (10.0 kB)\n",
      "Collecting flatten-dict<1,>=0.4.1 (from dvc)\n",
      "  Downloading flatten_dict-0.4.2-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting flufl.lock<10,>=8.1.0 (from dvc)\n",
      "  Downloading flufl_lock-9.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: fsspec>=2024.2.0 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from dvc) (2025.9.0)\n",
      "Collecting funcy>=1.14 (from dvc)\n",
      "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting grandalf<1,>=0.7 (from dvc)\n",
      "  Downloading grandalf-0.8-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting gto<2,>=1.6.0 (from dvc)\n",
      "  Downloading gto-1.9.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting hydra-core>=1.1 (from dvc)\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting iterative-telemetry>=0.0.7 (from dvc)\n",
      "  Downloading iterative_telemetry-0.0.10-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting kombu (from dvc)\n",
      "  Downloading kombu-5.6.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: networkx>=2.5 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from dvc) (3.5)\n",
      "Collecting omegaconf (from dvc)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pathspec<1,>=0.10.3 (from dvc)\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: platformdirs<5,>=3.1.1 in c:\\users\\swath\\appdata\\roaming\\python\\python314\\site-packages (from dvc) (4.7.0)\n",
      "Requirement already satisfied: psutil>=5.8 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from dvc) (7.1.3)\n",
      "Collecting pydot>=1.2.4 (from dvc)\n",
      "  Downloading pydot-4.0.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pygtrie>=2.3.2 (from dvc)\n",
      "  Downloading pygtrie-2.5.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting rich>=12 (from dvc)\n",
      "  Downloading rich-14.3.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting ruamel.yaml>=0.17.11 (from dvc)\n",
      "  Downloading ruamel_yaml-0.19.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting scmrepo<4,>=3.5.2 (from dvc)\n",
      "  Downloading scmrepo-3.6.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting shortuuid>=0.5 (from dvc)\n",
      "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting shtab<2,>=1.3.4 (from dvc)\n",
      "  Downloading shtab-1.8.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tabulate>=0.8.7 (from dvc)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting tomlkit>=0.11.1 (from dvc)\n",
      "  Downloading tomlkit-0.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting voluptuous>=0.11.7 (from dvc)\n",
      "  Downloading voluptuous-0.16.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting zc.lockfile>=1.2.1 (from dvc)\n",
      "  Downloading zc_lockfile-4.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting dictdiffer>=0.8.1 (from dvc-data<3.19.0,>=3.18.0->dvc)\n",
      "  Downloading dictdiffer-0.9.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting diskcache>=5.2.1 (from dvc-data<3.19.0,>=3.18.0->dvc)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting sqltrie<1,>=0.11.0 (from dvc-data<3.19.0,>=3.18.0->dvc)\n",
      "  Downloading sqltrie-0.11.2-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting orjson<4,>=3 (from dvc-data<3.19.0,>=3.18.0->dvc)\n",
      "  Downloading orjson-3.11.7-cp314-cp314-win_amd64.whl.metadata (43 kB)\n",
      "Collecting billiard<5.0,>=4.2.1 (from celery->dvc)\n",
      "  Downloading billiard-4.2.4-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting vine<6.0,>=5.1.0 (from celery->dvc)\n",
      "  Downloading vine-5.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting click-didyoumean>=0.3.0 (from celery->dvc)\n",
      "  Downloading click_didyoumean-0.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting click-repl>=0.2.0 (from celery->dvc)\n",
      "  Downloading click_repl-0.3.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting click-plugins>=1.1.1 (from celery->dvc)\n",
      "  Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting tzlocal (from celery->dvc)\n",
      "  Downloading tzlocal-5.3.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting atpublic (from flufl.lock<10,>=8.1.0->dvc)\n",
      "  Downloading atpublic-7.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting entrypoints (from gto<2,>=1.6.0->dvc)\n",
      "  Downloading entrypoints-0.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pydantic-settings>=2 (from gto<2,>=1.6.0->dvc)\n",
      "  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting semver>=2.13.0 (from gto<2,>=1.6.0->dvc)\n",
      "  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting typer>=0.4.1 (from gto<2,>=1.6.0->dvc)\n",
      "  Downloading typer-0.23.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting amqp<6.0.0,>=5.1.1 (from kombu->dvc)\n",
      "  Downloading amqp-5.3.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting pygit2>=1.14.0 (from scmrepo<4,>=3.5.2->dvc)\n",
      "  Downloading pygit2-1.19.1-cp314-cp314-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting asyncssh<3,>=2.13.1 (from scmrepo<4,>=3.5.2->dvc)\n",
      "  Downloading asyncssh-2.22.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting aiohttp-retry>=2.5.0 (from scmrepo<4,>=3.5.2->dvc)\n",
      "  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from aiohttp-retry>=2.5.0->scmrepo<4,>=3.5.2->dvc) (3.13.1)\n",
      "Collecting pycparser (from cffi>=2.0.0->cryptography<47,>=43.0.0->mlflow)\n",
      "  Downloading pycparser-3.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.36 in c:\\users\\swath\\appdata\\roaming\\python\\python314\\site-packages (from click-repl>=0.2.0->celery->dvc) (3.0.52)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->dvc)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting appdirs (from iterative-telemetry>=0.0.7->dvc)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from iterative-telemetry>=0.0.7->dvc) (3.19.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\swath\\appdata\\roaming\\python\\python314\\site-packages (from prettytable>=3.9->skops<1->mlflow) (0.6.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=12->dvc)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\swath\\appdata\\roaming\\python\\python314\\site-packages (from rich>=12->dvc) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=12->dvc)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.4.1->gto<2,>=1.6.0->dvc)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from zc.lockfile>=1.2.1->dvc) (70.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from aiohttp->aiohttp-retry>=2.5.0->scmrepo<4,>=3.5.2->dvc) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from aiohttp->aiohttp-retry>=2.5.0->scmrepo<4,>=3.5.2->dvc) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from aiohttp->aiohttp-retry>=2.5.0->scmrepo<4,>=3.5.2->dvc) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from aiohttp->aiohttp-retry>=2.5.0->scmrepo<4,>=3.5.2->dvc) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from aiohttp->aiohttp-retry>=2.5.0->scmrepo<4,>=3.5.2->dvc) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\swath\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from aiohttp->aiohttp-retry>=2.5.0->scmrepo<4,>=3.5.2->dvc) (1.22.0)\n",
      "Downloading mlflow-3.9.0-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 2.9/9.7 MB 15.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.7/9.7 MB 11.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.0/9.7 MB 9.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.2/9.7 MB 7.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.8/9.7 MB 5.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.3/9.7 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.8/9.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.6/9.7 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.4/9.7 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.2/9.7 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 4.4 MB/s  0:00:02\n",
      "Downloading mlflow_skinny-3.9.0-py3-none-any.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 1.0/2.8 MB 5.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.1/2.8 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 MB 5.5 MB/s  0:00:00\n",
      "Downloading mlflow_tracing-3.9.0-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 1.3/1.4 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 6.3 MB/s  0:00:00\n",
      "Downloading alembic-1.18.4-py3-none-any.whl (263 kB)\n",
      "Downloading cachetools-6.2.6-py3-none-any.whl (11 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading cloudpickle-3.1.2-py3-none-any.whl (22 kB)\n",
      "Downloading cryptography-46.0.5-cp311-abi3-win_amd64.whl (3.5 MB)\n",
      "   ---------------------------------------- 0.0/3.5 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 1.3/3.5 MB 7.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.9/3.5 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.5/3.5 MB 7.1 MB/s  0:00:00\n",
      "Downloading databricks_sdk-0.88.0-py3-none-any.whl (798 kB)\n",
      "   ---------------------------------------- 0.0/798.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 798.3/798.3 kB 5.3 MB/s  0:00:00\n",
      "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Downloading fastapi-0.129.0-py3-none-any.whl (102 kB)\n",
      "Downloading flask-3.1.2-py3-none-any.whl (103 kB)\n",
      "Downloading flask_cors-6.0.2-py3-none-any.whl (13 kB)\n",
      "Downloading gitpython-3.1.46-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading google_auth-2.48.0-py3-none-any.whl (236 kB)\n",
      "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "Downloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
      "Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading huey-2.6.0-py3-none-any.whl (76 kB)\n",
      "Downloading importlib_metadata-8.7.1-py3-none-any.whl (27 kB)\n",
      "Downloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
      "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n",
      "Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
      "Downloading protobuf-6.33.5-cp310-abi3-win_amd64.whl (437 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp314-cp314-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ------------------------------- -------- 1.6/2.0 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 8.1 MB/s  0:00:00\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading skops-0.13.0-py3-none-any.whl (131 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading sqlalchemy-2.0.46-cp314-cp314-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 9.0 MB/s  0:00:00\n",
      "Downloading sqlparse-0.5.5-py3-none-any.whl (46 kB)\n",
      "Downloading starlette-0.52.1-py3-none-any.whl (74 kB)\n",
      "Downloading uvicorn-0.40.0-py3-none-any.whl (68 kB)\n",
      "Downloading waitress-3.0.2-py3-none-any.whl (56 kB)\n",
      "Downloading dvc-3.66.1-py3-none-any.whl (469 kB)\n",
      "Downloading dpath-2.2.0-py3-none-any.whl (17 kB)\n",
      "Downloading dvc_data-3.18.2-py3-none-any.whl (79 kB)\n",
      "Downloading dvc_objects-5.2.0-py3-none-any.whl (33 kB)\n",
      "Downloading dvc_render-1.0.2-py3-none-any.whl (22 kB)\n",
      "Downloading dvc_studio_client-0.22.0-py3-none-any.whl (16 kB)\n",
      "Downloading dvc_task-0.40.2-py3-none-any.whl (21 kB)\n",
      "Downloading celery-5.6.2-py3-none-any.whl (445 kB)\n",
      "Downloading billiard-4.2.4-py3-none-any.whl (87 kB)\n",
      "Downloading flatten_dict-0.4.2-py2.py3-none-any.whl (9.7 kB)\n",
      "Downloading flufl_lock-9.0.0-py3-none-any.whl (11 kB)\n",
      "Downloading grandalf-0.8-py3-none-any.whl (41 kB)\n",
      "Downloading gto-1.9.0-py3-none-any.whl (45 kB)\n",
      "Downloading kombu-5.6.2-py3-none-any.whl (214 kB)\n",
      "Downloading vine-5.1.0-py3-none-any.whl (9.6 kB)\n",
      "Downloading amqp-5.3.1-py3-none-any.whl (50 kB)\n",
      "Downloading orjson-3.11.7-cp314-cp314-win_amd64.whl (124 kB)\n",
      "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading scmrepo-3.6.1-py3-none-any.whl (74 kB)\n",
      "Downloading asyncssh-2.22.0-py3-none-any.whl (374 kB)\n",
      "Downloading shtab-1.8.0-py3-none-any.whl (14 kB)\n",
      "Downloading sqltrie-0.11.2-py3-none-any.whl (17 kB)\n",
      "Downloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
      "Downloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cffi-2.0.0-cp314-cp314-win_amd64.whl (185 kB)\n",
      "Downloading click_didyoumean-0.3.1-py3-none-any.whl (3.6 kB)\n",
      "Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n",
      "Downloading click_repl-0.3.0-py3-none-any.whl (10 kB)\n",
      "Downloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\n",
      "Downloading dictdiffer-0.9.0-py2.py3-none-any.whl (16 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading dulwich-1.0.0-py3-none-any.whl (647 kB)\n",
      "   ---------------------------------------- 0.0/648.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 648.0/648.0 kB 6.8 MB/s  0:00:00\n",
      "Downloading dvc_http-2.32.0-py3-none-any.whl (12 kB)\n",
      "Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Downloading greenlet-3.3.1-cp314-cp314-win_amd64.whl (228 kB)\n",
      "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading iterative_telemetry-0.0.10-py3-none-any.whl (10 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading prettytable-3.17.0-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.2-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
      "Downloading pydot-4.0.1-py3-none-any.whl (37 kB)\n",
      "Downloading pygit2-1.19.1-cp314-cp314-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 8.8 MB/s  0:00:00\n",
      "Downloading pygtrie-2.5.0-py3-none-any.whl (25 kB)\n",
      "Downloading pywin32-311-cp314-cp314-win_amd64.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.1/9.7 MB 10.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.7/9.7 MB 9.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.2/9.7 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.7/9.7 MB 5.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.2/9.7 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.0/9.7 MB 4.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.1/9.7 MB 4.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.9/9.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.9/9.7 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 4.7 MB/s  0:00:02\n",
      "Downloading rich-14.3.2-py3-none-any.whl (309 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading ruamel_yaml-0.19.1-py3-none-any.whl (118 kB)\n",
      "Downloading semver-3.0.4-py3-none-any.whl (17 kB)\n",
      "Downloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading tomlkit-0.14.0-py3-none-any.whl (39 kB)\n",
      "Downloading typer-0.23.1-py3-none-any.whl (56 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading voluptuous-0.16.0-py3-none-any.whl (31 kB)\n",
      "Downloading werkzeug-3.1.5-py3-none-any.whl (225 kB)\n",
      "Downloading zc_lockfile-4.0-py3-none-any.whl (9.1 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading atpublic-7.0.0-py3-none-any.whl (6.4 kB)\n",
      "Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Downloading pycparser-3.0-py3-none-any.whl (48 kB)\n",
      "Downloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (pyproject.toml): started\n",
      "  Building wheel for antlr4-python3-runtime (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144617 sha256=3d60ab9871286166eb0679724a56d5c87ccc0e29ab808ad0a0ec9f85294c6550\n",
      "  Stored in directory: c:\\users\\swath\\appdata\\local\\pip\\cache\\wheels\\ea\\ca\\a6\\5ad163e6c66e23b25d4e6a81dba06bce77047ef6257d643728\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: pywin32, pygtrie, huey, funcy, dictdiffer, appdirs, antlr4-python3-runtime, zipp, zc.lockfile, werkzeug, waitress, voluptuous, vine, tzlocal, typing-inspection, tomlkit, tabulate, sqlparse, smmap, shtab, shortuuid, shellingham, semver, ruamel.yaml, python-dotenv, pydot, pydantic-core, pycparser, pyasn1, protobuf, prettytable, pathspec, orjson, omegaconf, mdurl, Mako, itsdangerous, greenlet, graphql-core, grandalf, flatten-dict, entrypoints, dvc-render, dvc-objects, dulwich, dpath, distro, diskcache, configobj, cloudpickle, click, cachetools, blinker, billiard, atpublic, annotated-types, annotated-doc, uvicorn, starlette, sqltrie, sqlalchemy, rsa, pydantic, pyasn1-modules, opentelemetry-proto, markdown-it-py, iterative-telemetry, importlib_metadata, hydra-core, graphql-relay, gitdb, flufl.lock, Flask, dvc-studio-client, docker, click-repl, click-plugins, click-didyoumean, cffi, amqp, skops, rich, pygit2, pydantic-settings, opentelemetry-api, kombu, graphene, gitpython, Flask-CORS, fastapi, dvc-data, cryptography, alembic, aiohttp-retry, typer, opentelemetry-semantic-conventions, google-auth, dvc-http, celery, asyncssh, scmrepo, opentelemetry-sdk, dvc-task, databricks-sdk, mlflow-tracing, mlflow-skinny, gto, mlflow, dvc\n",
      "\n",
      "   ----------------------------------------   0/109 [pywin32]\n",
      "   ----------------------------------------   0/109 [pywin32]\n",
      "   ----------------------------------------   0/109 [pywin32]\n",
      "   ----------------------------------------   0/109 [pywin32]\n",
      "   ----------------------------------------   0/109 [pywin32]\n",
      "   ----------------------------------------   0/109 [pywin32]\n",
      "   ----------------------------------------   0/109 [pywin32]\n",
      "   ----------------------------------------   0/109 [pywin32]\n",
      "   ----------------------------------------   0/109 [pywin32]\n",
      "   ----------------------------------------   0/109 [pywin32]\n",
      "   ----------------------------------------   1/109 [pygtrie]\n",
      "    ---------------------------------------   2/109 [huey]\n",
      "   - --------------------------------------   4/109 [dictdiffer]\n",
      "   -- -------------------------------------   6/109 [antlr4-python3-runtime]\n",
      "   -- -------------------------------------   8/109 [zc.lockfile]\n",
      "   --- ------------------------------------   9/109 [werkzeug]\n",
      "   --- ------------------------------------  10/109 [waitress]\n",
      "   ----- ----------------------------------  15/109 [tomlkit]\n",
      "   ------ ---------------------------------  17/109 [sqlparse]\n",
      "   -------- -------------------------------  23/109 [ruamel.yaml]\n",
      "   -------- -------------------------------  23/109 [ruamel.yaml]\n",
      "   --------- ------------------------------  26/109 [pydantic-core]\n",
      "   ---------- -----------------------------  28/109 [pyasn1]\n",
      "   ---------- -----------------------------  29/109 [protobuf]\n",
      "   ----------- ----------------------------  30/109 [prettytable]\n",
      "   ------------ ---------------------------  33/109 [omegaconf]\n",
      "   ------------ ---------------------------  35/109 [Mako]\n",
      "   ------------- --------------------------  38/109 [graphql-core]\n",
      "   ------------- --------------------------  38/109 [graphql-core]\n",
      "   ------------- --------------------------  38/109 [graphql-core]\n",
      "   --------------- ------------------------  42/109 [dvc-render]\n",
      "   ---------------- -----------------------  44/109 [dulwich]\n",
      "   ---------------- -----------------------  44/109 [dulwich]\n",
      "   ---------------- -----------------------  44/109 [dulwich]\n",
      "   ---------------- -----------------------  44/109 [dulwich]\n",
      "   ---------------- -----------------------  44/109 [dulwich]\n",
      "   ---------------- -----------------------  44/109 [dulwich]\n",
      "   ----------------- ----------------------  49/109 [cloudpickle]\n",
      "   ------------------ ---------------------  51/109 [cachetools]\n",
      "   ------------------- --------------------  53/109 [billiard]\n",
      "   -------------------- -------------------  57/109 [uvicorn]\n",
      "   --------------------- ------------------  58/109 [starlette]\n",
      "   ---------------------- -----------------  60/109 [sqlalchemy]\n",
      "   ---------------------- -----------------  60/109 [sqlalchemy]\n",
      "   ---------------------- -----------------  60/109 [sqlalchemy]\n",
      "   ---------------------- -----------------  60/109 [sqlalchemy]\n",
      "   ---------------------- -----------------  60/109 [sqlalchemy]\n",
      "   ---------------------- -----------------  60/109 [sqlalchemy]\n",
      "   ---------------------- -----------------  60/109 [sqlalchemy]\n",
      "   ---------------------- -----------------  60/109 [sqlalchemy]\n",
      "   ---------------------- -----------------  60/109 [sqlalchemy]\n",
      "   ---------------------- -----------------  60/109 [sqlalchemy]\n",
      "   ---------------------- -----------------  61/109 [rsa]\n",
      "   ---------------------- -----------------  62/109 [pydantic]\n",
      "   ---------------------- -----------------  62/109 [pydantic]\n",
      "   ---------------------- -----------------  62/109 [pydantic]\n",
      "   ----------------------- ----------------  63/109 [pyasn1-modules]\n",
      "   ----------------------- ----------------  63/109 [pyasn1-modules]\n",
      "   ----------------------- ----------------  64/109 [opentelemetry-proto]\n",
      "   ----------------------- ----------------  65/109 [markdown-it-py]\n",
      "   ------------------------ ---------------  68/109 [hydra-core]\n",
      "   ------------------------ ---------------  68/109 [hydra-core]\n",
      "   ------------------------- --------------  70/109 [gitdb]\n",
      "   -------------------------- -------------  73/109 [dvc-studio-client]\n",
      "   --------------------------- ------------  74/109 [docker]\n",
      "   ---------------------------- -----------  78/109 [cffi]\n",
      "   ----------------------------- ----------  80/109 [skops]\n",
      "   ----------------------------- ----------  81/109 [rich]\n",
      "   ----------------------------- ----------  81/109 [rich]\n",
      "   ----------------------------- ----------  81/109 [rich]\n",
      "   ------------------------------ ---------  82/109 [pygit2]\n",
      "   ------------------------------ ---------  84/109 [opentelemetry-api]\n",
      "   ------------------------------- --------  85/109 [kombu]\n",
      "   ------------------------------- --------  85/109 [kombu]\n",
      "   ------------------------------- --------  86/109 [graphene]\n",
      "   ------------------------------- --------  86/109 [graphene]\n",
      "   ------------------------------- --------  87/109 [gitpython]\n",
      "   ------------------------------- --------  87/109 [gitpython]\n",
      "   -------------------------------- -------  89/109 [fastapi]\n",
      "   --------------------------------- ------  90/109 [dvc-data]\n",
      "   --------------------------------- ------  91/109 [cryptography]\n",
      "   --------------------------------- ------  91/109 [cryptography]\n",
      "   --------------------------------- ------  92/109 [alembic]\n",
      "   --------------------------------- ------  92/109 [alembic]\n",
      "   ---------------------------------- -----  94/109 [typer]\n",
      "   --------------------------- ---  95/109 [opentelemetry-semantic-conventions]\n",
      "   ----------------------------------- ----  96/109 [google-auth]\n",
      "   ----------------------------------- ----  96/109 [google-auth]\n",
      "   ----------------------------------- ----  98/109 [celery]\n",
      "   ----------------------------------- ----  98/109 [celery]\n",
      "   ----------------------------------- ----  98/109 [celery]\n",
      "   ----------------------------------- ----  98/109 [celery]\n",
      "   ------------------------------------ ---  99/109 [asyncssh]\n",
      "   ------------------------------------ ---  99/109 [asyncssh]\n",
      "   ------------------------------------ --- 100/109 [scmrepo]\n",
      "   ------------------------------------- -- 101/109 [opentelemetry-sdk]\n",
      "   ------------------------------------- -- 103/109 [databricks-sdk]\n",
      "   ------------------------------------- -- 103/109 [databricks-sdk]\n",
      "   ------------------------------------- -- 103/109 [databricks-sdk]\n",
      "   ------------------------------------- -- 103/109 [databricks-sdk]\n",
      "   ------------------------------------- -- 103/109 [databricks-sdk]\n",
      "   ------------------------------------- -- 103/109 [databricks-sdk]\n",
      "   ------------------------------------- -- 103/109 [databricks-sdk]\n",
      "   -------------------------------------- - 104/109 [mlflow-tracing]\n",
      "   -------------------------------------- - 104/109 [mlflow-tracing]\n",
      "   -------------------------------------- - 104/109 [mlflow-tracing]\n",
      "   -------------------------------------- - 104/109 [mlflow-tracing]\n",
      "   -------------------------------------- - 104/109 [mlflow-tracing]\n",
      "   -------------------------------------- - 104/109 [mlflow-tracing]\n",
      "   -------------------------------------- - 104/109 [mlflow-tracing]\n",
      "   -------------------------------------- - 104/109 [mlflow-tracing]\n",
      "   -------------------------------------- - 104/109 [mlflow-tracing]\n",
      "   -------------------------------------- - 104/109 [mlflow-tracing]\n",
      "   -------------------------------------- - 104/109 [mlflow-tracing]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 105/109 [mlflow-skinny]\n",
      "   -------------------------------------- - 106/109 [gto]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  107/109 [mlflow]\n",
      "   ---------------------------------------  108/109 [dvc]\n",
      "   ---------------------------------------  108/109 [dvc]\n",
      "   ---------------------------------------  108/109 [dvc]\n",
      "   ---------------------------------------  108/109 [dvc]\n",
      "   ---------------------------------------  108/109 [dvc]\n",
      "   ---------------------------------------  108/109 [dvc]\n",
      "   ---------------------------------------- 109/109 [dvc]\n",
      "\n",
      "Successfully installed Flask-3.1.2 Flask-CORS-6.0.2 Mako-1.3.10 aiohttp-retry-2.9.1 alembic-1.18.4 amqp-5.3.1 annotated-doc-0.0.4 annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 appdirs-1.4.4 asyncssh-2.22.0 atpublic-7.0.0 billiard-4.2.4 blinker-1.9.0 cachetools-6.2.6 celery-5.6.2 cffi-2.0.0 click-8.3.1 click-didyoumean-0.3.1 click-plugins-1.1.1.2 click-repl-0.3.0 cloudpickle-3.1.2 configobj-5.0.9 cryptography-46.0.5 databricks-sdk-0.88.0 dictdiffer-0.9.0 diskcache-5.6.3 distro-1.9.0 docker-7.1.0 dpath-2.2.0 dulwich-1.0.0 dvc-3.66.1 dvc-data-3.18.2 dvc-http-2.32.0 dvc-objects-5.2.0 dvc-render-1.0.2 dvc-studio-client-0.22.0 dvc-task-0.40.2 entrypoints-0.4 fastapi-0.129.0 flatten-dict-0.4.2 flufl.lock-9.0.0 funcy-2.0 gitdb-4.0.12 gitpython-3.1.46 google-auth-2.48.0 grandalf-0.8 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 greenlet-3.3.1 gto-1.9.0 huey-2.6.0 hydra-core-1.3.2 importlib_metadata-8.7.1 iterative-telemetry-0.0.10 itsdangerous-2.2.0 kombu-5.6.2 markdown-it-py-4.0.0 mdurl-0.1.2 mlflow-3.9.0 mlflow-skinny-3.9.0 mlflow-tracing-3.9.0 omegaconf-2.3.0 opentelemetry-api-1.39.1 opentelemetry-proto-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 orjson-3.11.7 pathspec-0.12.1 prettytable-3.17.0 protobuf-6.33.5 pyasn1-0.6.2 pyasn1-modules-0.4.2 pycparser-3.0 pydantic-2.12.5 pydantic-core-2.41.5 pydantic-settings-2.12.0 pydot-4.0.1 pygit2-1.19.1 pygtrie-2.5.0 python-dotenv-1.2.1 pywin32-311 rich-14.3.2 rsa-4.9.1 ruamel.yaml-0.19.1 scmrepo-3.6.1 semver-3.0.4 shellingham-1.5.4 shortuuid-1.0.13 shtab-1.8.0 skops-0.13.0 smmap-5.0.2 sqlalchemy-2.0.46 sqlparse-0.5.5 sqltrie-0.11.2 starlette-0.52.1 tabulate-0.9.0 tomlkit-0.14.0 typer-0.23.1 typing-inspection-0.4.2 tzlocal-5.3.1 uvicorn-0.40.0 vine-5.1.0 voluptuous-0.16.0 waitress-3.0.2 werkzeug-3.1.5 zc.lockfile-4.0 zipp-3.23.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pywin32_postinstall.exe and pywin32_testall.exe are installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script huey_consumer.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script waitress-serve.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tabulate.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script sqlformat.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script shtab.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script shortuuid.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script pysemver.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script dotenv.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script mako-render.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script dulwich.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script distro.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script uvicorn.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown-it.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script flask.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script fastapi.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script dvc-data.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script alembic.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script typer.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script celery.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script mlflow.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script gto.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script mlflow.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script dvc.exe is installed in 'c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# Run this cell first to install all dependencies\n",
    "# Choose the appropriate PyTorch installation based on your system:\n",
    "\n",
    "# For CUDA 11.8 (NVIDIA GPU):\n",
    "!python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# For CPU only, use this instead:\n",
    "# !python -m pip install torch torchvision torchaudio\n",
    "\n",
    "# Install other required packages:\n",
    "!python -m pip install mlflow dvc numpy pandas matplotlib seaborn scikit-learn pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d6ca08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch version: 2.9.0+cu130\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e579824",
   "metadata": {},
   "source": [
    "## 2. Project Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff847dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  raw_data_path: C:\\Users\\swath\\dataset\\archive (2)\\PetImages\n",
      "  project_root: C:\\Users\\swath\\dataset\\mlops_project\n",
      "  processed_data_path: C:\\Users\\swath\\dataset\\mlops_project\\data\\processed\n",
      "  models_path: C:\\Users\\swath\\dataset\\mlops_project\\models\n",
      "  experiments_path: C:\\Users\\swath\\dataset\\mlops_project\\experiments\n",
      "  img_size: 224\n",
      "  batch_size: 32\n",
      "  num_epochs: 10\n",
      "  learning_rate: 0.001\n",
      "  train_split: 0.8\n",
      "  val_split: 0.1\n",
      "  test_split: 0.1\n",
      "  experiment_name: cats_dogs_classification\n",
      "  run_name: baseline_cnn_20260213_211032\n"
     ]
    }
   ],
   "source": [
    "# Project paths and configuration\n",
    "# NOTE: For team collaboration, each member should update these paths for their local setup\n",
    "# OR use environment variables (recommended for teams)\n",
    "\n",
    "# Option 1: Use environment variables (recommended)\n",
    "# Set these in your terminal before running:\n",
    "# For Windows: $env:DATASET_PATH = \"C:\\path\\to\\PetImages\"\n",
    "# For Linux/Mac: export DATASET_PATH=\"/path/to/PetImages\"\n",
    "\n",
    "import os\n",
    "\n",
    "# Try to get path from environment variable, fallback to default\n",
    "DEFAULT_DATA_PATH = r'C:\\Users\\swath\\dataset\\archive (2)\\PetImages'\n",
    "raw_data_path = os.environ.get('DATASET_PATH', DEFAULT_DATA_PATH)\n",
    "\n",
    "# Use relative path for project files (works for everyone)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', 'mlops_project'))\n",
    "\n",
    "CONFIG = {\n",
    "    'raw_data_path': raw_data_path,\n",
    "    'project_root': project_root,\n",
    "    'processed_data_path': os.path.join(project_root, 'data', 'processed'),\n",
    "    'models_path': os.path.join(project_root, 'models'),\n",
    "    'experiments_path': os.path.join(project_root, 'experiments'),\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    'img_size': 224,\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 0.001,\n",
    "    'train_split': 0.8,\n",
    "    'val_split': 0.1,\n",
    "    'test_split': 0.1,\n",
    "    \n",
    "    # MLflow tracking\n",
    "    'experiment_name': 'cats_dogs_classification',\n",
    "    'run_name': f'baseline_cnn_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(CONFIG['processed_data_path'], exist_ok=True)\n",
    "os.makedirs(CONFIG['models_path'], exist_ok=True)\n",
    "os.makedirs(CONFIG['experiments_path'], exist_ok=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEAM SETUP INSTRUCTIONS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"Each team member should either:\")\n",
    "print(\"1. Set environment variable DATASET_PATH to their local dataset location\")\n",
    "print(\"2. OR modify DEFAULT_DATA_PATH in this cell to match their local path\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c59f24",
   "metadata": {},
   "source": [
    "## Task 1: Data & Code Versioning\n",
    "\n",
    "### Git for Source Code\n",
    "Initialize Git repository to version control all code, scripts, and notebooks.\n",
    "```bash\n",
    "git init\n",
    "git add .\n",
    "git commit -m \"Initial commit\"\n",
    "```\n",
    "\n",
    "### DVC for Dataset Versioning\n",
    "Track large datasets and preprocessed data using DVC (Data Version Control).\n",
    "\n",
    "**For Team Collaboration:**\n",
    "- Team Lead: Setup DVC remote storage (Google Drive, S3, etc.)\n",
    "- Team Members: Use `dvc pull` to download dataset automatically\n",
    "- No hardcoded paths needed! DVC handles data sharing across team\n",
    "\n",
    "```bash\n",
    "# Team Lead\n",
    "dvc remote add -d myremote gdrive://FOLDER_ID\n",
    "dvc add path/to/PetImages\n",
    "dvc push\n",
    "git add PetImages.dvc .gitignore\n",
    "git commit -m \"Track dataset with DVC\"\n",
    "\n",
    "# Team Members  \n",
    "dvc pull  # Dataset automatically downloaded!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86956a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " DVC metadata created\n",
      "Dataset: Kaggle Cats and Dogs\n",
      "Total samples: 24203\n",
      "Cat samples: 12500\n",
      "Dog samples: 11703\n"
     ]
    }
   ],
   "source": [
    "# DVC Setup for Data Versioning\n",
    "# Run these commands in terminal:\n",
    "# cd C:\\Users\\swath\\dataset\\mlops_project\n",
    "# git init\n",
    "# dvc init\n",
    "# dvc add ../archive\\ (2)/PetImages\n",
    "# git add PetImages.dvc .gitignore\n",
    "# git commit -m \"Initial data version with DVC\"\n",
    "\n",
    "# Create DVC metadata file\n",
    "dvc_metadata = {\n",
    "    'dataset': 'Kaggle Cats and Dogs',\n",
    "    'source_path': CONFIG['raw_data_path'],\n",
    "    'total_samples': 24203,\n",
    "    'classes': ['Cat', 'Dog'],\n",
    "    'cat_samples': 12500,\n",
    "    'dog_samples': 11703,\n",
    "    'version': '1.0',\n",
    "    'created_at': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(os.path.join(CONFIG['project_root'], 'dataset_metadata.json'), 'w') as f:\n",
    "    json.dump(dvc_metadata, f, indent=2)\n",
    "\n",
    "print(\" Data versioning setup completed\")\n",
    "print(f\"Dataset: {dvc_metadata['dataset']}\")\n",
    "print(f\"Total samples: {dvc_metadata['total_samples']}\")\n",
    "print(f\"Classes: {', '.join(dvc_metadata['classes'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1df6ac",
   "metadata": {},
   "source": [
    "## Data Loading & Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e56dd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cat images: 12499\n",
      "Number of dog images: 12499\n",
      "Total images: 24998\n"
     ]
    }
   ],
   "source": [
    "# Count images per class\n",
    "cat_path = os.path.join(CONFIG['raw_data_path'], 'Cat')\n",
    "dog_path = os.path.join(CONFIG['raw_data_path'], 'Dog')\n",
    "\n",
    "cat_images = [f for f in os.listdir(cat_path) if f.endswith('.jpg')]\n",
    "dog_images = [f for f in os.listdir(dog_path) if f.endswith('.jpg')]\n",
    "\n",
    "print(f\"Number of cat images: {len(cat_images)}\")\n",
    "print(f\"Number of dog images: {len(dog_images)}\")\n",
    "print(f\"Total images: {len(cat_images) + len(dog_images)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d578bc7",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Augmentation (224x224 RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706b6e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Augmentation Pipeline:\n",
      "  Training:\n",
      "    - Resize to 224x224\n",
      "    - Random Horizontal Flip (p=0.5)\n",
      "    - Random Rotation (15)\n",
      "    - Color Jitter (brightness, contrast, saturation)\n",
      "    - ImageNet Normalization\n",
      "\n",
      "  Validation/Test:\n",
      "    - Resize to 224x224\n",
      "    - ImageNet Normalization\n",
      "\n",
      " Transforms defined\n"
     ]
    }
   ],
   "source": [
    "# Define transforms for training (with augmentation) and validation/test\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Data Augmentation Pipeline:\")\n",
    "print(\"  Training:\")\n",
    "print(f\"    - Resize to {CONFIG['img_size']}x{CONFIG['img_size']}\")\n",
    "print(\"    - Random Horizontal Flip (p=0.5)\")\n",
    "print(\"    - Random Rotation (15)\")\n",
    "print(\"    - Color Jitter (brightness, contrast, saturation)\")\n",
    "print(\"    - ImageNet Normalization\")\n",
    "print(\"\\n  Validation/Test:\")\n",
    "print(f\"    - Resize to {CONFIG['img_size']}x{CONFIG['img_size']}\")\n",
    "print(\"    - ImageNet Normalization\")\n",
    "print(\"\\n Transforms defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ee432",
   "metadata": {},
   "source": [
    "## Train/Validation/Test Split (80/10/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92f6977e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples loaded: 24998\n",
      "\n",
      "Dataset Split (80/10/10):\n",
      "  Train: 19998 samples (80%)\n",
      "  Validation: 2499 samples (10%)\n",
      "  Test: 2501 samples (10%)\n",
      "\n",
      "Data Loaders Created:\n",
      "  Train batches: 625\n",
      "  Val batches: 79\n",
      "  Test batches: 79\n",
      "  Batch size: 32\n",
      "\n",
      " Dataset split completed\n"
     ]
    }
   ],
   "source": [
    "# Custom dataset class\n",
    "class CatsDogsDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, class_to_idx={'Cat': 0, 'Dog': 1}):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.samples = []\n",
    "        \n",
    "        # Load all samples\n",
    "        for class_name, class_idx in class_to_idx.items():\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.exists(class_dir):\n",
    "                for img_file in os.listdir(class_dir):\n",
    "                    if img_file.endswith('.jpg'):\n",
    "                        self.samples.append((os.path.join(class_dir, img_file), class_idx))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            # Return a blank image if corrupted\n",
    "            return torch.zeros(3, CONFIG['img_size'], CONFIG['img_size']), label\n",
    "\n",
    "# Load full dataset\n",
    "full_dataset = CatsDogsDataset(CONFIG['raw_data_path'], transform=train_transforms)\n",
    "print(f\"Total samples loaded: {len(full_dataset)}\")\n",
    "\n",
    "# Calculate split sizes\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(CONFIG['train_split'] * total_size)\n",
    "val_size = int(CONFIG['val_split'] * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "print(f\"\\nDataset Split (80/10/10):\")\n",
    "print(f\"  Train: {train_size} samples ({CONFIG['train_split']*100:.0f}%)\")\n",
    "print(f\"  Validation: {val_size} samples ({CONFIG['val_split']*100:.0f}%)\")\n",
    "print(f\"  Test: {test_size} samples ({CONFIG['test_split']*100:.0f}%)\")\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, \n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\nData Loaders Created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(\"\\n Dataset split completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b201f",
   "metadata": {},
   "source": [
    "## Task 2: Model Building - Baseline CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98793016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline CNN Architecture:\n",
      "BaselineCNN(\n",
      "  (conv_block1): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_block2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_block3): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_block4): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=50176, out_features=512, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.3, inplace=False)\n",
      "    (7): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Model Statistics:\n",
      "  Total parameters: 26,145,922\n",
      "  Trainable parameters: 26,145,922\n",
      "  Model size: ~99.74 MB (FP32)\n",
      "\n",
      " Model initialized\n"
     ]
    }
   ],
   "source": [
    "# Define baseline CNN architecture\n",
    "class BaselineCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(BaselineCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # 224 -> 112\n",
    "        )\n",
    "        \n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # 112 -> 56\n",
    "        )\n",
    "        \n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # 56 -> 28\n",
    "        )\n",
    "        \n",
    "        self.conv_block4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # 28 -> 14\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 14 * 14, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        x = self.conv_block4(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = BaselineCNN(num_classes=2).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Baseline CNN Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / (1024**2):.2f} MB (FP32)\")\n",
    "print(\"\\n Model initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f157569",
   "metadata": {},
   "source": [
    "## Task 3: Experiment Tracking with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5659798b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:178: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance. For migrating existing data, https://github.com/mlflow/mlflow-export-import can be used.\n",
      "  return FileStore(store_uri, store_uri)\n",
      "2026/02/13 21:11:20 INFO mlflow.tracking.fluent: Experiment with name 'cats_dogs_classification' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Configuration:\n",
      "  Tracking URI: file:///C:\\Users\\swath\\dataset\\mlops_project\\experiments\n",
      "  Experiment: cats_dogs_classification\n",
      "  Run name: baseline_cnn_20260213_211032\n",
      "\n",
      " MLflow initialized\n",
      "\n",
      "Dataset info to be logged:\n",
      "  Total samples: 24998\n",
      "  Train/Val/Test: 19998/2499/2501\n",
      "  Image size: 224x224\n"
     ]
    }
   ],
   "source": [
    "# Setup MLflow\n",
    "mlflow.set_tracking_uri(f\"file:///{CONFIG['experiments_path']}\")\n",
    "mlflow.set_experiment(CONFIG['experiment_name'])\n",
    "\n",
    "print(\"MLflow Configuration:\")\n",
    "print(f\"  Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"  Experiment: {CONFIG['experiment_name']}\")\n",
    "print(f\"  Run name: {CONFIG['run_name']}\")\n",
    "print(\"\\n MLflow initialized\")\n",
    "\n",
    "# Log dataset information\n",
    "print(\"\\nDataset info to be logged:\")\n",
    "print(f\"  Total samples: {total_size}\")\n",
    "print(f\"  Train/Val/Test: {train_size}/{val_size}/{test_size}\")\n",
    "print(f\"  Image size: {CONFIG['img_size']}x{CONFIG['img_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52f01c1",
   "metadata": {},
   "source": [
    "## Model Training with MLflow Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d54cb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Loss function: CrossEntropyLoss\n",
      "  Optimizer: Adam\n",
      "  Learning rate: 0.001\n",
      "  LR Scheduler: ReduceLROnPlateau (factor=0.5, patience=2)\n",
      "  Number of epochs: 10\n",
      "  Device: cuda\n",
      "\n",
      " Training setup completed\n"
     ]
    }
   ],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Loss function: CrossEntropyLoss\")\n",
    "print(f\"  Optimizer: Adam\")\n",
    "print(f\"  Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"  LR Scheduler: ReduceLROnPlateau (factor=0.5, patience=2)\")\n",
    "print(f\"  Number of epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(\"\\n Training setup completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06c04010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training and validation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Training and validation functions\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training', leave=False)\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': loss.item(), 'acc': 100 * correct / total})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc='Validation', leave=False)\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': loss.item(), 'acc': 100 * correct / total})\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\" Training and validation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a246f007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Starting Training - Run ID: 400a118275cf433f98b0a1e87794bed7\n",
      "============================================================\n",
      "\n",
      "\n",
      "Epoch 1/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6328 | Train Acc: 64.40%\n",
      "Val Loss:   0.5720 | Val Acc:   72.11%\n",
      " Best model saved (val_loss: 0.5720)\n",
      "\n",
      "Epoch 2/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5676 | Train Acc: 71.16%\n",
      "Val Loss:   0.5333 | Val Acc:   73.95%\n",
      " Best model saved (val_loss: 0.5333)\n",
      "\n",
      "Epoch 3/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5245 | Train Acc: 74.18%\n",
      "Val Loss:   0.5411 | Val Acc:   72.31%\n",
      "\n",
      "Epoch 4/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4986 | Train Acc: 76.25%\n",
      "Val Loss:   0.4799 | Val Acc:   77.79%\n",
      " Best model saved (val_loss: 0.4799)\n",
      "\n",
      "Epoch 5/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4503 | Train Acc: 79.59%\n",
      "Val Loss:   0.4680 | Val Acc:   79.03%\n",
      " Best model saved (val_loss: 0.4680)\n",
      "\n",
      "Epoch 6/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4162 | Train Acc: 81.73%\n",
      "Val Loss:   0.4637 | Val Acc:   78.59%\n",
      " Best model saved (val_loss: 0.4637)\n",
      "\n",
      "Epoch 7/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3758 | Train Acc: 83.42%\n",
      "Val Loss:   0.3778 | Val Acc:   84.59%\n",
      " Best model saved (val_loss: 0.3778)\n",
      "\n",
      "Epoch 8/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3469 | Train Acc: 85.23%\n",
      "Val Loss:   0.3266 | Val Acc:   86.15%\n",
      " Best model saved (val_loss: 0.3266)\n",
      "\n",
      "Epoch 9/10\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m40\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m train_loss, train_acc = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m     44\u001b[39m val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m      6\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m      8\u001b[39m pbar = tqdm(loader, desc=\u001b[33m'\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m'\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mCatsDogsDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     23\u001b[39m     image = Image.open(img_path).convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m         image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# Return a blank image if corrupted\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:1276\u001b[39m, in \u001b[36mColorJitter.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m   1274\u001b[39m     img = F.adjust_brightness(img, brightness_factor)\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fn_id == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m contrast_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1276\u001b[39m     img = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43madjust_contrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fn_id == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m saturation_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1278\u001b[39m     img = F.adjust_saturation(img, saturation_factor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torchvision\\transforms\\functional.py:907\u001b[39m, in \u001b[36madjust_contrast\u001b[39m\u001b[34m(img, contrast_factor)\u001b[39m\n\u001b[32m    905\u001b[39m     _log_api_usage_once(adjust_contrast)\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43madjust_contrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrast_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F_t.adjust_contrast(img, contrast_factor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:85\u001b[39m, in \u001b[36madjust_contrast\u001b[39m\u001b[34m(img, contrast_factor)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pil_image(img):\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mimg should be PIL Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m enhancer = \u001b[43mImageEnhance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mContrast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m img = enhancer.enhance(contrast_factor)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\PIL\\ImageEnhance.py:75\u001b[39m, in \u001b[36mContrast.__init__\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m image.mode != \u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     74\u001b[39m     image = image.convert(\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m mean = \u001b[38;5;28mint\u001b[39m(\u001b[43mImageStat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mStat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m.mean[\u001b[32m0\u001b[39m] + \u001b[32m0.5\u001b[39m)\n\u001b[32m     76\u001b[39m \u001b[38;5;28mself\u001b[39m.degenerate = Image.new(\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m, image.size, mean)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.degenerate.mode != \u001b[38;5;28mself\u001b[39m.image.mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\PIL\\ImageStat.py:54\u001b[39m, in \u001b[36mStat.__init__\u001b[39m\u001b[34m(self, image_or_list, mask)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03mCalculate statistics for the given image. If a mask is included,\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03monly the regions covered by that mask are included in the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m \u001b[33;03m:param mask: An optional mask.\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image_or_list, Image.Image):\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[38;5;28mself\u001b[39m.h = \u001b[43mimage_or_list\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image_or_list, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     56\u001b[39m     \u001b[38;5;28mself\u001b[39m.h = image_or_list\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\swath\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\PIL\\Image.py:1703\u001b[39m, in \u001b[36mImage.histogram\u001b[39m\u001b[34m(self, mask, extrema)\u001b[39m\n\u001b[32m   1699\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1700\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.im.histogram(\n\u001b[32m   1701\u001b[39m         extrema \u001b[38;5;28;01mif\u001b[39;00m extrema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.getextrema()\n\u001b[32m   1702\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1703\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Main training loop with MLflow logging\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=CONFIG['run_name']) as run:\n",
    "    # Store run ID for later use\n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "    # Log hyperparameters\n",
    "    mlflow.log_params({\n",
    "        'model_architecture': 'BaselineCNN',\n",
    "        'img_size': CONFIG['img_size'],\n",
    "        'batch_size': CONFIG['batch_size'],\n",
    "        'learning_rate': CONFIG['learning_rate'],\n",
    "        'num_epochs': CONFIG['num_epochs'],\n",
    "        'optimizer': 'Adam',\n",
    "        'loss_function': 'CrossEntropyLoss',\n",
    "        'train_samples': train_size,\n",
    "        'val_samples': val_size,\n",
    "        'test_samples': test_size,\n",
    "        'total_params': total_params\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Starting Training - Run ID: {run_id}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = os.path.join(CONFIG['models_path'], 'best_model.pt')\n",
    "    \n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{CONFIG['num_epochs']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metrics({\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_acc,\n",
    "            'learning_rate': optimizer.param_groups[0]['lr']\n",
    "        }, step=epoch)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc\n",
    "            }, best_model_path)\n",
    "            print(f\" Best model saved (val_loss: {val_loss:.4f})\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training Completed!\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Best model saved to: {best_model_path}\")\n",
    "    \n",
    "    # Log final model to MLflow\n",
    "    mlflow.pytorch.log_model(model, \"model\")\n",
    "    mlflow.log_artifact(best_model_path)\n",
    "    \n",
    "    print(\"\\n Model logged to MLflow\")\n",
    "\n",
    "# Store run_id for later cells\n",
    "print(f\"\\nRun ID stored: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11de0445",
   "metadata": {},
   "source": [
    "## Artifacts: Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a546ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o', linewidth=2)\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(history['train_acc'], label='Train Accuracy', marker='o', linewidth=2)\n",
    "axes[1].plot(history['val_acc'], label='Val Accuracy', marker='s', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "loss_curves_path = os.path.join(CONFIG['experiments_path'], 'training_curves.png')\n",
    "plt.savefig(loss_curves_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Log to MLflow\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    mlflow.log_artifact(loss_curves_path)\n",
    "\n",
    "print(f\"\\n Training curves saved and logged to MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d86e0e",
   "metadata": {},
   "source": [
    "## Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca0c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"Evaluating best model on test set...\")\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']+1}\")\n",
    "print(f\"Validation accuracy: {checkpoint['val_acc']:.2f}%\\n\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = validate_epoch(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# Log test metrics\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    mlflow.log_metrics({\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_acc\n",
    "    })\n",
    "\n",
    "print(\"\\n Test metrics logged to MLflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca2ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for confusion matrix\n",
    "def get_predictions(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc='Generating predictions'):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "test_preds, test_labels = get_predictions(model, test_loader, device)\n",
    "print(f\"\\nGenerated {len(test_preds)} predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f95d2ae",
   "metadata": {},
   "source": [
    "## Artifacts: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8454f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "class_names = ['Cat', 'Dog']\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Add percentages\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        percentage = cm[i, j] / cm[i].sum() * 100\n",
    "        plt.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)', \n",
    "                ha='center', va='center', fontsize=10, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "cm_path = os.path.join(CONFIG['experiments_path'], 'confusion_matrix.png')\n",
    "plt.savefig(cm_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Log to MLflow\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    mlflow.log_artifact(cm_path)\n",
    "\n",
    "print(\"\\n Confusion matrix generated and logged\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(test_labels, test_preds, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9693945",
   "metadata": {},
   "source": [
    "## Model Serialization (.pt, .pkl formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e826403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in multiple serialized formats\n",
    "print(\"Serializing trained model...\\n\")\n",
    "\n",
    "# 1. PyTorch format (.pt)\n",
    "pt_model_path = os.path.join(CONFIG['models_path'], 'baseline_cnn.pt')\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_architecture': 'BaselineCNN',\n",
    "    'img_size': CONFIG['img_size'],\n",
    "    'num_classes': 2,\n",
    "    'test_accuracy': test_acc,\n",
    "    'class_names': class_names\n",
    "}, pt_model_path)\n",
    "print(f\" PyTorch model (.pt) saved: {pt_model_path}\")\n",
    "print(f\"  Size: {os.path.getsize(pt_model_path) / (1024**2):.2f} MB\")\n",
    "\n",
    "# 2. Pickle format (.pkl)\n",
    "pkl_model_path = os.path.join(CONFIG['models_path'], 'baseline_cnn.pkl')\n",
    "with open(pkl_model_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'model': model,\n",
    "        'config': CONFIG,\n",
    "        'class_names': class_names,\n",
    "        'test_accuracy': test_acc\n",
    "    }, f)\n",
    "print(f\"\\n Pickle model (.pkl) saved: {pkl_model_path}\")\n",
    "print(f\"  Size: {os.path.getsize(pkl_model_path) / (1024**2):.2f} MB\")\n",
    "\n",
    "# Log model artifacts to MLflow\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    mlflow.log_artifact(pt_model_path)\n",
    "    mlflow.log_artifact(pkl_model_path)\n",
    "\n",
    "print(\"\\n All model artifacts logged to MLflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a8a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M1 Completion Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"M1: MODEL DEVELOPMENT & EXPERIMENT TRACKING - COMPLETED\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n Task 1: Data & Code Versioning (Git + DVC)\")\n",
    "print(\" Task 2: Model Building (Baseline CNN)\")\n",
    "print(\" Task 3: Experiment Tracking (MLflow)\")\n",
    "print(f\"\\nModels saved: {CONFIG['models_path']}\")\n",
    "print(f\"Experiments: {CONFIG['experiments_path']}\")\n",
    "print(f\"\\nView MLflow UI:\")\n",
    "print(f\"  mlflow ui --backend-store-uri file:///{CONFIG['experiments_path']}\")\n",
    "print(\"  Open: http://localhost:5000\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
